{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1962,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01529051987767584,
      "grad_norm": 11.022989273071289,
      "learning_rate": 1.9908256880733945e-05,
      "loss": 12.0231,
      "step": 10
    },
    {
      "epoch": 0.03058103975535168,
      "grad_norm": 155.26002502441406,
      "learning_rate": 1.980632008154944e-05,
      "loss": 13.7919,
      "step": 20
    },
    {
      "epoch": 0.045871559633027525,
      "grad_norm": 8.996129035949707,
      "learning_rate": 1.9704383282364936e-05,
      "loss": 13.1686,
      "step": 30
    },
    {
      "epoch": 0.06116207951070336,
      "grad_norm": 141.7963104248047,
      "learning_rate": 1.960244648318043e-05,
      "loss": 13.3416,
      "step": 40
    },
    {
      "epoch": 0.0764525993883792,
      "grad_norm": 44.24433517456055,
      "learning_rate": 1.9500509683995926e-05,
      "loss": 13.182,
      "step": 50
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 13.48249340057373,
      "learning_rate": 1.9398572884811417e-05,
      "loss": 13.0575,
      "step": 60
    },
    {
      "epoch": 0.10703363914373089,
      "grad_norm": 88.3427505493164,
      "learning_rate": 1.9296636085626912e-05,
      "loss": 12.4913,
      "step": 70
    },
    {
      "epoch": 0.12232415902140673,
      "grad_norm": 172.0810546875,
      "learning_rate": 1.9194699286442407e-05,
      "loss": 11.2284,
      "step": 80
    },
    {
      "epoch": 0.13761467889908258,
      "grad_norm": 149.57150268554688,
      "learning_rate": 1.9092762487257902e-05,
      "loss": 11.1828,
      "step": 90
    },
    {
      "epoch": 0.1529051987767584,
      "grad_norm": 16.117151260375977,
      "learning_rate": 1.8990825688073394e-05,
      "loss": 10.2954,
      "step": 100
    },
    {
      "epoch": 0.16819571865443425,
      "grad_norm": 20.483734130859375,
      "learning_rate": 1.888888888888889e-05,
      "loss": 9.0832,
      "step": 110
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 26.03529167175293,
      "learning_rate": 1.8786952089704384e-05,
      "loss": 9.9902,
      "step": 120
    },
    {
      "epoch": 0.19877675840978593,
      "grad_norm": 51.66054916381836,
      "learning_rate": 1.868501529051988e-05,
      "loss": 8.1638,
      "step": 130
    },
    {
      "epoch": 0.21406727828746178,
      "grad_norm": 163.2643280029297,
      "learning_rate": 1.8583078491335374e-05,
      "loss": 8.1312,
      "step": 140
    },
    {
      "epoch": 0.22935779816513763,
      "grad_norm": 52.623809814453125,
      "learning_rate": 1.848114169215087e-05,
      "loss": 7.9207,
      "step": 150
    },
    {
      "epoch": 0.24464831804281345,
      "grad_norm": 32.053035736083984,
      "learning_rate": 1.837920489296636e-05,
      "loss": 6.3753,
      "step": 160
    },
    {
      "epoch": 0.2599388379204893,
      "grad_norm": 79.20103454589844,
      "learning_rate": 1.8277268093781856e-05,
      "loss": 5.6334,
      "step": 170
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 27.640329360961914,
      "learning_rate": 1.817533129459735e-05,
      "loss": 4.9015,
      "step": 180
    },
    {
      "epoch": 0.290519877675841,
      "grad_norm": 27.43526268005371,
      "learning_rate": 1.8073394495412846e-05,
      "loss": 4.2102,
      "step": 190
    },
    {
      "epoch": 0.3058103975535168,
      "grad_norm": 17.84138298034668,
      "learning_rate": 1.797145769622834e-05,
      "loss": 4.2758,
      "step": 200
    },
    {
      "epoch": 0.3211009174311927,
      "grad_norm": 17.423583984375,
      "learning_rate": 1.7869520897043836e-05,
      "loss": 4.203,
      "step": 210
    },
    {
      "epoch": 0.3363914373088685,
      "grad_norm": 14.66960334777832,
      "learning_rate": 1.7767584097859328e-05,
      "loss": 2.4859,
      "step": 220
    },
    {
      "epoch": 0.3516819571865443,
      "grad_norm": 17.27591896057129,
      "learning_rate": 1.7665647298674823e-05,
      "loss": 2.6796,
      "step": 230
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 10.101943016052246,
      "learning_rate": 1.7563710499490318e-05,
      "loss": 2.3312,
      "step": 240
    },
    {
      "epoch": 0.382262996941896,
      "grad_norm": 14.564958572387695,
      "learning_rate": 1.746177370030581e-05,
      "loss": 1.9515,
      "step": 250
    },
    {
      "epoch": 0.39755351681957185,
      "grad_norm": 3.7078545093536377,
      "learning_rate": 1.7359836901121305e-05,
      "loss": 1.8833,
      "step": 260
    },
    {
      "epoch": 0.41284403669724773,
      "grad_norm": 3.6452858448028564,
      "learning_rate": 1.72579001019368e-05,
      "loss": 1.0571,
      "step": 270
    },
    {
      "epoch": 0.42813455657492355,
      "grad_norm": 2.051708221435547,
      "learning_rate": 1.7155963302752295e-05,
      "loss": 0.5912,
      "step": 280
    },
    {
      "epoch": 0.4434250764525994,
      "grad_norm": 1.3715555667877197,
      "learning_rate": 1.705402650356779e-05,
      "loss": 0.5931,
      "step": 290
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 0.9812724590301514,
      "learning_rate": 1.6952089704383285e-05,
      "loss": 0.5488,
      "step": 300
    },
    {
      "epoch": 0.4740061162079511,
      "grad_norm": 0.8181449770927429,
      "learning_rate": 1.6850152905198776e-05,
      "loss": 0.4319,
      "step": 310
    },
    {
      "epoch": 0.4892966360856269,
      "grad_norm": 0.8397002816200256,
      "learning_rate": 1.674821610601427e-05,
      "loss": 0.3806,
      "step": 320
    },
    {
      "epoch": 0.5045871559633027,
      "grad_norm": 0.7191246747970581,
      "learning_rate": 1.6646279306829766e-05,
      "loss": 0.3467,
      "step": 330
    },
    {
      "epoch": 0.5198776758409785,
      "grad_norm": 0.7541691064834595,
      "learning_rate": 1.654434250764526e-05,
      "loss": 0.315,
      "step": 340
    },
    {
      "epoch": 0.5351681957186545,
      "grad_norm": 0.7385095953941345,
      "learning_rate": 1.6442405708460757e-05,
      "loss": 0.3024,
      "step": 350
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 0.7344053387641907,
      "learning_rate": 1.634046890927625e-05,
      "loss": 0.2487,
      "step": 360
    },
    {
      "epoch": 0.5657492354740061,
      "grad_norm": 0.7197365164756775,
      "learning_rate": 1.6238532110091743e-05,
      "loss": 0.214,
      "step": 370
    },
    {
      "epoch": 0.581039755351682,
      "grad_norm": 0.7380478382110596,
      "learning_rate": 1.6136595310907238e-05,
      "loss": 0.1807,
      "step": 380
    },
    {
      "epoch": 0.5963302752293578,
      "grad_norm": 0.7074641585350037,
      "learning_rate": 1.6034658511722733e-05,
      "loss": 0.1453,
      "step": 390
    },
    {
      "epoch": 0.6116207951070336,
      "grad_norm": 1.5917141437530518,
      "learning_rate": 1.593272171253823e-05,
      "loss": 0.1116,
      "step": 400
    },
    {
      "epoch": 0.6269113149847095,
      "grad_norm": 0.6585666537284851,
      "learning_rate": 1.5830784913353723e-05,
      "loss": 0.0789,
      "step": 410
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 0.5678913593292236,
      "learning_rate": 1.572884811416922e-05,
      "loss": 0.0525,
      "step": 420
    },
    {
      "epoch": 0.6574923547400612,
      "grad_norm": 0.4791237413883209,
      "learning_rate": 1.5626911314984713e-05,
      "loss": 0.0349,
      "step": 430
    },
    {
      "epoch": 0.672782874617737,
      "grad_norm": 0.337268203496933,
      "learning_rate": 1.5524974515800205e-05,
      "loss": 0.0244,
      "step": 440
    },
    {
      "epoch": 0.6880733944954128,
      "grad_norm": 0.7171729803085327,
      "learning_rate": 1.54230377166157e-05,
      "loss": 0.0192,
      "step": 450
    },
    {
      "epoch": 0.7033639143730887,
      "grad_norm": 0.24195055663585663,
      "learning_rate": 1.5321100917431192e-05,
      "loss": 0.0151,
      "step": 460
    },
    {
      "epoch": 0.7186544342507645,
      "grad_norm": 0.22292931377887726,
      "learning_rate": 1.5219164118246687e-05,
      "loss": 0.0126,
      "step": 470
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 0.20235422253608704,
      "learning_rate": 1.5117227319062182e-05,
      "loss": 0.01,
      "step": 480
    },
    {
      "epoch": 0.7492354740061162,
      "grad_norm": 0.18060870468616486,
      "learning_rate": 1.5015290519877677e-05,
      "loss": 0.0092,
      "step": 490
    },
    {
      "epoch": 0.764525993883792,
      "grad_norm": 0.16068263351917267,
      "learning_rate": 1.491335372069317e-05,
      "loss": 0.007,
      "step": 500
    },
    {
      "epoch": 0.7798165137614679,
      "grad_norm": 0.13932248950004578,
      "learning_rate": 1.4811416921508665e-05,
      "loss": 0.0055,
      "step": 510
    },
    {
      "epoch": 0.7951070336391437,
      "grad_norm": 0.11116492003202438,
      "learning_rate": 1.470948012232416e-05,
      "loss": 0.0047,
      "step": 520
    },
    {
      "epoch": 0.8103975535168195,
      "grad_norm": 0.11900023370981216,
      "learning_rate": 1.4607543323139655e-05,
      "loss": 0.0038,
      "step": 530
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 0.08618087321519852,
      "learning_rate": 1.4505606523955149e-05,
      "loss": 0.0033,
      "step": 540
    },
    {
      "epoch": 0.8409785932721713,
      "grad_norm": 0.08319692313671112,
      "learning_rate": 1.4403669724770644e-05,
      "loss": 0.0036,
      "step": 550
    },
    {
      "epoch": 0.8562691131498471,
      "grad_norm": 0.07178600877523422,
      "learning_rate": 1.4301732925586139e-05,
      "loss": 0.0025,
      "step": 560
    },
    {
      "epoch": 0.8715596330275229,
      "grad_norm": 0.060127392411231995,
      "learning_rate": 1.4199796126401632e-05,
      "loss": 0.0024,
      "step": 570
    },
    {
      "epoch": 0.8868501529051988,
      "grad_norm": 0.0678093209862709,
      "learning_rate": 1.4097859327217127e-05,
      "loss": 0.0022,
      "step": 580
    },
    {
      "epoch": 0.9021406727828746,
      "grad_norm": 0.05049557238817215,
      "learning_rate": 1.3995922528032622e-05,
      "loss": 0.002,
      "step": 590
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 0.05127404257655144,
      "learning_rate": 1.3893985728848116e-05,
      "loss": 0.0018,
      "step": 600
    },
    {
      "epoch": 0.9327217125382263,
      "grad_norm": 0.04587528854608536,
      "learning_rate": 1.379204892966361e-05,
      "loss": 0.0016,
      "step": 610
    },
    {
      "epoch": 0.9480122324159022,
      "grad_norm": 0.043915510177612305,
      "learning_rate": 1.3690112130479106e-05,
      "loss": 0.0015,
      "step": 620
    },
    {
      "epoch": 0.963302752293578,
      "grad_norm": 0.04058119282126427,
      "learning_rate": 1.3588175331294597e-05,
      "loss": 0.0015,
      "step": 630
    },
    {
      "epoch": 0.9785932721712538,
      "grad_norm": 0.040871698409318924,
      "learning_rate": 1.3486238532110092e-05,
      "loss": 0.0013,
      "step": 640
    },
    {
      "epoch": 0.9938837920489296,
      "grad_norm": 6.0444111824035645,
      "learning_rate": 1.3384301732925586e-05,
      "loss": 0.0041,
      "step": 650
    },
    {
      "epoch": 1.0091743119266054,
      "grad_norm": 0.0319955050945282,
      "learning_rate": 1.328236493374108e-05,
      "loss": 0.0012,
      "step": 660
    },
    {
      "epoch": 1.0244648318042813,
      "grad_norm": 0.03555909916758537,
      "learning_rate": 1.3180428134556576e-05,
      "loss": 0.0011,
      "step": 670
    },
    {
      "epoch": 1.039755351681957,
      "grad_norm": 0.030057964846491814,
      "learning_rate": 1.307849133537207e-05,
      "loss": 0.0011,
      "step": 680
    },
    {
      "epoch": 1.0550458715596331,
      "grad_norm": 0.02930895984172821,
      "learning_rate": 1.2976554536187564e-05,
      "loss": 0.0011,
      "step": 690
    },
    {
      "epoch": 1.070336391437309,
      "grad_norm": 0.027481280267238617,
      "learning_rate": 1.287461773700306e-05,
      "loss": 0.001,
      "step": 700
    },
    {
      "epoch": 1.0856269113149848,
      "grad_norm": 0.025659389793872833,
      "learning_rate": 1.2772680937818553e-05,
      "loss": 0.001,
      "step": 710
    },
    {
      "epoch": 1.1009174311926606,
      "grad_norm": 0.023793024942278862,
      "learning_rate": 1.2670744138634048e-05,
      "loss": 0.0009,
      "step": 720
    },
    {
      "epoch": 1.1162079510703364,
      "grad_norm": 0.023502003401517868,
      "learning_rate": 1.2568807339449543e-05,
      "loss": 0.0009,
      "step": 730
    },
    {
      "epoch": 1.1314984709480123,
      "grad_norm": 0.029705986380577087,
      "learning_rate": 1.2466870540265038e-05,
      "loss": 0.0009,
      "step": 740
    },
    {
      "epoch": 1.146788990825688,
      "grad_norm": 0.021893218159675598,
      "learning_rate": 1.2364933741080531e-05,
      "loss": 0.0009,
      "step": 750
    },
    {
      "epoch": 1.162079510703364,
      "grad_norm": 0.02114271931350231,
      "learning_rate": 1.2262996941896026e-05,
      "loss": 0.0008,
      "step": 760
    },
    {
      "epoch": 1.1773700305810397,
      "grad_norm": 0.02491748332977295,
      "learning_rate": 1.2161060142711521e-05,
      "loss": 0.0008,
      "step": 770
    },
    {
      "epoch": 1.1926605504587156,
      "grad_norm": 0.023754911497235298,
      "learning_rate": 1.2059123343527015e-05,
      "loss": 0.0007,
      "step": 780
    },
    {
      "epoch": 1.2079510703363914,
      "grad_norm": 0.01951880380511284,
      "learning_rate": 1.195718654434251e-05,
      "loss": 0.0007,
      "step": 790
    },
    {
      "epoch": 1.2232415902140672,
      "grad_norm": 0.01987084187567234,
      "learning_rate": 1.1855249745158005e-05,
      "loss": 0.0007,
      "step": 800
    },
    {
      "epoch": 1.238532110091743,
      "grad_norm": 0.0195015836507082,
      "learning_rate": 1.1753312945973498e-05,
      "loss": 0.0007,
      "step": 810
    },
    {
      "epoch": 1.2538226299694188,
      "grad_norm": 0.017946472391486168,
      "learning_rate": 1.1651376146788991e-05,
      "loss": 0.0006,
      "step": 820
    },
    {
      "epoch": 1.2691131498470947,
      "grad_norm": 0.018603157252073288,
      "learning_rate": 1.1549439347604485e-05,
      "loss": 0.0006,
      "step": 830
    },
    {
      "epoch": 1.2844036697247707,
      "grad_norm": 0.016581816598773003,
      "learning_rate": 1.144750254841998e-05,
      "loss": 0.0006,
      "step": 840
    },
    {
      "epoch": 1.2996941896024465,
      "grad_norm": 0.015787791460752487,
      "learning_rate": 1.1345565749235475e-05,
      "loss": 0.0006,
      "step": 850
    },
    {
      "epoch": 1.3149847094801224,
      "grad_norm": 0.021128060296177864,
      "learning_rate": 1.1243628950050968e-05,
      "loss": 0.0006,
      "step": 860
    },
    {
      "epoch": 1.3302752293577982,
      "grad_norm": 0.029093297198414803,
      "learning_rate": 1.1141692150866463e-05,
      "loss": 0.0006,
      "step": 870
    },
    {
      "epoch": 1.345565749235474,
      "grad_norm": 0.014009644277393818,
      "learning_rate": 1.1039755351681958e-05,
      "loss": 0.0005,
      "step": 880
    },
    {
      "epoch": 1.3608562691131498,
      "grad_norm": 0.014151031151413918,
      "learning_rate": 1.0937818552497452e-05,
      "loss": 0.0006,
      "step": 890
    },
    {
      "epoch": 1.3761467889908257,
      "grad_norm": 0.026300912722945213,
      "learning_rate": 1.0835881753312947e-05,
      "loss": 0.0005,
      "step": 900
    },
    {
      "epoch": 1.3914373088685015,
      "grad_norm": 0.02053004503250122,
      "learning_rate": 1.0733944954128442e-05,
      "loss": 0.0005,
      "step": 910
    },
    {
      "epoch": 1.4067278287461773,
      "grad_norm": 0.013330142945051193,
      "learning_rate": 1.0632008154943935e-05,
      "loss": 0.0005,
      "step": 920
    },
    {
      "epoch": 1.4220183486238533,
      "grad_norm": 0.01429784670472145,
      "learning_rate": 1.053007135575943e-05,
      "loss": 0.0005,
      "step": 930
    },
    {
      "epoch": 1.4373088685015292,
      "grad_norm": 0.012813059613108635,
      "learning_rate": 1.0428134556574925e-05,
      "loss": 0.0005,
      "step": 940
    },
    {
      "epoch": 1.452599388379205,
      "grad_norm": 0.012756419368088245,
      "learning_rate": 1.032619775739042e-05,
      "loss": 0.0008,
      "step": 950
    },
    {
      "epoch": 1.4678899082568808,
      "grad_norm": 0.03186206892132759,
      "learning_rate": 1.0224260958205913e-05,
      "loss": 0.0005,
      "step": 960
    },
    {
      "epoch": 1.4831804281345566,
      "grad_norm": 0.013465682044625282,
      "learning_rate": 1.0122324159021408e-05,
      "loss": 0.0004,
      "step": 970
    },
    {
      "epoch": 1.4984709480122325,
      "grad_norm": 0.013659958727657795,
      "learning_rate": 1.0020387359836904e-05,
      "loss": 0.0004,
      "step": 980
    },
    {
      "epoch": 1.5137614678899083,
      "grad_norm": 0.01184915192425251,
      "learning_rate": 9.918450560652397e-06,
      "loss": 0.0045,
      "step": 990
    },
    {
      "epoch": 1.529051987767584,
      "grad_norm": 0.01341851707547903,
      "learning_rate": 9.81651376146789e-06,
      "loss": 0.0004,
      "step": 1000
    },
    {
      "epoch": 1.54434250764526,
      "grad_norm": 0.012447740882635117,
      "learning_rate": 9.714576962283385e-06,
      "loss": 0.0004,
      "step": 1010
    },
    {
      "epoch": 1.5596330275229358,
      "grad_norm": 0.014241088181734085,
      "learning_rate": 9.61264016309888e-06,
      "loss": 0.0004,
      "step": 1020
    },
    {
      "epoch": 1.5749235474006116,
      "grad_norm": 0.015965444967150688,
      "learning_rate": 9.510703363914374e-06,
      "loss": 0.0004,
      "step": 1030
    },
    {
      "epoch": 1.5902140672782874,
      "grad_norm": 0.011438843794167042,
      "learning_rate": 9.408766564729869e-06,
      "loss": 0.0004,
      "step": 1040
    },
    {
      "epoch": 1.6055045871559632,
      "grad_norm": 0.013023270294070244,
      "learning_rate": 9.306829765545362e-06,
      "loss": 0.0004,
      "step": 1050
    },
    {
      "epoch": 1.620795107033639,
      "grad_norm": 0.01416064240038395,
      "learning_rate": 9.204892966360857e-06,
      "loss": 0.0004,
      "step": 1060
    },
    {
      "epoch": 1.6360856269113149,
      "grad_norm": 0.01107722893357277,
      "learning_rate": 9.10295616717635e-06,
      "loss": 0.0004,
      "step": 1070
    },
    {
      "epoch": 1.6513761467889907,
      "grad_norm": 0.010181847028434277,
      "learning_rate": 9.001019367991845e-06,
      "loss": 0.0004,
      "step": 1080
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.011222156696021557,
      "learning_rate": 8.89908256880734e-06,
      "loss": 0.0004,
      "step": 1090
    },
    {
      "epoch": 1.6819571865443423,
      "grad_norm": 0.011280720122158527,
      "learning_rate": 8.797145769622834e-06,
      "loss": 0.0004,
      "step": 1100
    },
    {
      "epoch": 1.6972477064220184,
      "grad_norm": 0.010504381731152534,
      "learning_rate": 8.695208970438329e-06,
      "loss": 0.0004,
      "step": 1110
    },
    {
      "epoch": 1.7125382262996942,
      "grad_norm": 0.010950583033263683,
      "learning_rate": 8.593272171253824e-06,
      "loss": 0.0004,
      "step": 1120
    },
    {
      "epoch": 1.72782874617737,
      "grad_norm": 0.012429505586624146,
      "learning_rate": 8.491335372069319e-06,
      "loss": 0.0003,
      "step": 1130
    },
    {
      "epoch": 1.7431192660550459,
      "grad_norm": 0.012413672171533108,
      "learning_rate": 8.389398572884812e-06,
      "loss": 0.0003,
      "step": 1140
    },
    {
      "epoch": 1.7584097859327217,
      "grad_norm": 0.009762225672602654,
      "learning_rate": 8.287461773700306e-06,
      "loss": 0.0003,
      "step": 1150
    },
    {
      "epoch": 1.7737003058103975,
      "grad_norm": 0.024060431867837906,
      "learning_rate": 8.1855249745158e-06,
      "loss": 0.0003,
      "step": 1160
    },
    {
      "epoch": 1.7889908256880735,
      "grad_norm": 0.013152371160686016,
      "learning_rate": 8.083588175331296e-06,
      "loss": 0.0003,
      "step": 1170
    },
    {
      "epoch": 1.8042813455657494,
      "grad_norm": 0.008623664267361164,
      "learning_rate": 7.981651376146789e-06,
      "loss": 0.0003,
      "step": 1180
    },
    {
      "epoch": 1.8195718654434252,
      "grad_norm": 0.008839460089802742,
      "learning_rate": 7.879714576962284e-06,
      "loss": 0.0003,
      "step": 1190
    },
    {
      "epoch": 1.834862385321101,
      "grad_norm": 0.011806725524365902,
      "learning_rate": 7.77777777777778e-06,
      "loss": 0.0003,
      "step": 1200
    },
    {
      "epoch": 1.8501529051987768,
      "grad_norm": 0.010293346829712391,
      "learning_rate": 7.675840978593273e-06,
      "loss": 0.0003,
      "step": 1210
    },
    {
      "epoch": 1.8654434250764527,
      "grad_norm": 0.00845242664217949,
      "learning_rate": 7.5739041794087676e-06,
      "loss": 0.0003,
      "step": 1220
    },
    {
      "epoch": 1.8807339449541285,
      "grad_norm": 0.008457960560917854,
      "learning_rate": 7.471967380224262e-06,
      "loss": 0.0003,
      "step": 1230
    },
    {
      "epoch": 1.8960244648318043,
      "grad_norm": 0.008671374060213566,
      "learning_rate": 7.370030581039755e-06,
      "loss": 0.0003,
      "step": 1240
    },
    {
      "epoch": 1.9113149847094801,
      "grad_norm": 0.008446973748505116,
      "learning_rate": 7.26809378185525e-06,
      "loss": 0.0003,
      "step": 1250
    },
    {
      "epoch": 1.926605504587156,
      "grad_norm": 0.008769064210355282,
      "learning_rate": 7.166156982670744e-06,
      "loss": 0.0003,
      "step": 1260
    },
    {
      "epoch": 1.9418960244648318,
      "grad_norm": 0.007957087829709053,
      "learning_rate": 7.0642201834862385e-06,
      "loss": 0.0003,
      "step": 1270
    },
    {
      "epoch": 1.9571865443425076,
      "grad_norm": 0.010166460648179054,
      "learning_rate": 6.9622833843017336e-06,
      "loss": 0.0003,
      "step": 1280
    },
    {
      "epoch": 1.9724770642201834,
      "grad_norm": 0.00764585193246603,
      "learning_rate": 6.860346585117228e-06,
      "loss": 0.0003,
      "step": 1290
    },
    {
      "epoch": 1.9877675840978593,
      "grad_norm": 0.007096851244568825,
      "learning_rate": 6.758409785932723e-06,
      "loss": 0.0003,
      "step": 1300
    },
    {
      "epoch": 2.003058103975535,
      "grad_norm": 0.008808137848973274,
      "learning_rate": 6.656472986748217e-06,
      "loss": 0.0003,
      "step": 1310
    },
    {
      "epoch": 2.018348623853211,
      "grad_norm": 0.008203786797821522,
      "learning_rate": 6.554536187563711e-06,
      "loss": 0.0003,
      "step": 1320
    },
    {
      "epoch": 2.0336391437308867,
      "grad_norm": 0.008930819109082222,
      "learning_rate": 6.452599388379206e-06,
      "loss": 0.0003,
      "step": 1330
    },
    {
      "epoch": 2.0489296636085625,
      "grad_norm": 0.00876663438975811,
      "learning_rate": 6.3506625891947e-06,
      "loss": 0.0003,
      "step": 1340
    },
    {
      "epoch": 2.0642201834862384,
      "grad_norm": 0.007474594283849001,
      "learning_rate": 6.248725790010194e-06,
      "loss": 0.0003,
      "step": 1350
    },
    {
      "epoch": 2.079510703363914,
      "grad_norm": 0.006740494165569544,
      "learning_rate": 6.146788990825688e-06,
      "loss": 0.0003,
      "step": 1360
    },
    {
      "epoch": 2.09480122324159,
      "grad_norm": 0.006866662297397852,
      "learning_rate": 6.044852191641183e-06,
      "loss": 0.0003,
      "step": 1370
    },
    {
      "epoch": 2.1100917431192663,
      "grad_norm": 0.007447042036801577,
      "learning_rate": 5.942915392456677e-06,
      "loss": 0.0003,
      "step": 1380
    },
    {
      "epoch": 2.1253822629969417,
      "grad_norm": 0.017705006524920464,
      "learning_rate": 5.840978593272172e-06,
      "loss": 0.0003,
      "step": 1390
    },
    {
      "epoch": 2.140672782874618,
      "grad_norm": 0.006952355150133371,
      "learning_rate": 5.7390417940876664e-06,
      "loss": 0.0003,
      "step": 1400
    },
    {
      "epoch": 2.1559633027522938,
      "grad_norm": 0.008305107243359089,
      "learning_rate": 5.637104994903161e-06,
      "loss": 0.0003,
      "step": 1410
    },
    {
      "epoch": 2.1712538226299696,
      "grad_norm": 0.006899562198668718,
      "learning_rate": 5.535168195718656e-06,
      "loss": 0.0003,
      "step": 1420
    },
    {
      "epoch": 2.1865443425076454,
      "grad_norm": 0.007111112587153912,
      "learning_rate": 5.433231396534149e-06,
      "loss": 0.0006,
      "step": 1430
    },
    {
      "epoch": 2.2018348623853212,
      "grad_norm": 0.007976163178682327,
      "learning_rate": 5.331294597349643e-06,
      "loss": 0.0002,
      "step": 1440
    },
    {
      "epoch": 2.217125382262997,
      "grad_norm": 0.009313675574958324,
      "learning_rate": 5.229357798165137e-06,
      "loss": 0.0002,
      "step": 1450
    },
    {
      "epoch": 2.232415902140673,
      "grad_norm": 0.009179753251373768,
      "learning_rate": 5.1274209989806325e-06,
      "loss": 0.0003,
      "step": 1460
    },
    {
      "epoch": 2.2477064220183487,
      "grad_norm": 0.009620276279747486,
      "learning_rate": 5.025484199796127e-06,
      "loss": 0.0002,
      "step": 1470
    },
    {
      "epoch": 2.2629969418960245,
      "grad_norm": 0.007998868823051453,
      "learning_rate": 4.923547400611622e-06,
      "loss": 0.0002,
      "step": 1480
    },
    {
      "epoch": 2.2782874617737003,
      "grad_norm": 0.007188121788203716,
      "learning_rate": 4.821610601427116e-06,
      "loss": 0.0002,
      "step": 1490
    },
    {
      "epoch": 2.293577981651376,
      "grad_norm": 0.007146777585148811,
      "learning_rate": 4.71967380224261e-06,
      "loss": 0.0002,
      "step": 1500
    },
    {
      "epoch": 2.308868501529052,
      "grad_norm": 0.0075195664539933205,
      "learning_rate": 4.617737003058104e-06,
      "loss": 0.0002,
      "step": 1510
    },
    {
      "epoch": 2.324159021406728,
      "grad_norm": 0.006805185694247484,
      "learning_rate": 4.5158002038735985e-06,
      "loss": 0.0002,
      "step": 1520
    },
    {
      "epoch": 2.3394495412844036,
      "grad_norm": 0.007481246255338192,
      "learning_rate": 4.4138634046890935e-06,
      "loss": 0.0002,
      "step": 1530
    },
    {
      "epoch": 2.3547400611620795,
      "grad_norm": 0.00734594976529479,
      "learning_rate": 4.311926605504588e-06,
      "loss": 0.0002,
      "step": 1540
    },
    {
      "epoch": 2.3700305810397553,
      "grad_norm": 0.006403041072189808,
      "learning_rate": 4.209989806320082e-06,
      "loss": 0.0003,
      "step": 1550
    },
    {
      "epoch": 2.385321100917431,
      "grad_norm": 0.008057794533669949,
      "learning_rate": 4.108053007135576e-06,
      "loss": 0.0002,
      "step": 1560
    },
    {
      "epoch": 2.400611620795107,
      "grad_norm": 0.006341214757412672,
      "learning_rate": 4.00611620795107e-06,
      "loss": 0.0002,
      "step": 1570
    },
    {
      "epoch": 2.4159021406727827,
      "grad_norm": 0.006721049081534147,
      "learning_rate": 3.904179408766565e-06,
      "loss": 0.0002,
      "step": 1580
    },
    {
      "epoch": 2.4311926605504586,
      "grad_norm": 0.006440990138798952,
      "learning_rate": 3.8022426095820595e-06,
      "loss": 0.0002,
      "step": 1590
    },
    {
      "epoch": 2.4464831804281344,
      "grad_norm": 0.02429267205297947,
      "learning_rate": 3.7003058103975537e-06,
      "loss": 0.0002,
      "step": 1600
    },
    {
      "epoch": 2.46177370030581,
      "grad_norm": 0.006295457016676664,
      "learning_rate": 3.5983690112130483e-06,
      "loss": 0.0002,
      "step": 1610
    },
    {
      "epoch": 2.477064220183486,
      "grad_norm": 0.0064895558170974255,
      "learning_rate": 3.4964322120285425e-06,
      "loss": 0.0002,
      "step": 1620
    },
    {
      "epoch": 2.4923547400611623,
      "grad_norm": 0.012097573839128017,
      "learning_rate": 3.394495412844037e-06,
      "loss": 0.0002,
      "step": 1630
    },
    {
      "epoch": 2.5076452599388377,
      "grad_norm": 0.007992099039256573,
      "learning_rate": 3.292558613659531e-06,
      "loss": 0.0002,
      "step": 1640
    },
    {
      "epoch": 2.522935779816514,
      "grad_norm": 0.005968439858406782,
      "learning_rate": 3.1906218144750255e-06,
      "loss": 0.0002,
      "step": 1650
    },
    {
      "epoch": 2.5382262996941893,
      "grad_norm": 0.005507097579538822,
      "learning_rate": 3.08868501529052e-06,
      "loss": 0.0002,
      "step": 1660
    },
    {
      "epoch": 2.5535168195718656,
      "grad_norm": 0.006680510472506285,
      "learning_rate": 2.9867482161060148e-06,
      "loss": 0.0002,
      "step": 1670
    },
    {
      "epoch": 2.5688073394495414,
      "grad_norm": 0.005910768639296293,
      "learning_rate": 2.884811416921509e-06,
      "loss": 0.0002,
      "step": 1680
    },
    {
      "epoch": 2.5840978593272173,
      "grad_norm": 0.006722418125718832,
      "learning_rate": 2.782874617737003e-06,
      "loss": 0.0002,
      "step": 1690
    },
    {
      "epoch": 2.599388379204893,
      "grad_norm": 0.00609006779268384,
      "learning_rate": 2.6809378185524978e-06,
      "loss": 0.0002,
      "step": 1700
    },
    {
      "epoch": 2.614678899082569,
      "grad_norm": 0.006588238291442394,
      "learning_rate": 2.579001019367992e-06,
      "loss": 0.0002,
      "step": 1710
    },
    {
      "epoch": 2.6299694189602447,
      "grad_norm": 0.006022559944540262,
      "learning_rate": 2.4770642201834866e-06,
      "loss": 0.0002,
      "step": 1720
    },
    {
      "epoch": 2.6452599388379205,
      "grad_norm": 0.007355445064604282,
      "learning_rate": 2.3751274209989808e-06,
      "loss": 0.0002,
      "step": 1730
    },
    {
      "epoch": 2.6605504587155964,
      "grad_norm": 0.00632967846468091,
      "learning_rate": 2.2731906218144754e-06,
      "loss": 0.0002,
      "step": 1740
    },
    {
      "epoch": 2.675840978593272,
      "grad_norm": 0.007150357589125633,
      "learning_rate": 2.1712538226299696e-06,
      "loss": 0.0002,
      "step": 1750
    },
    {
      "epoch": 2.691131498470948,
      "grad_norm": 0.0071861655451357365,
      "learning_rate": 2.0693170234454642e-06,
      "loss": 0.0002,
      "step": 1760
    },
    {
      "epoch": 2.706422018348624,
      "grad_norm": 0.0065870825201272964,
      "learning_rate": 1.9673802242609584e-06,
      "loss": 0.0002,
      "step": 1770
    },
    {
      "epoch": 2.7217125382262997,
      "grad_norm": 0.0064813788048923016,
      "learning_rate": 1.8654434250764528e-06,
      "loss": 0.0002,
      "step": 1780
    },
    {
      "epoch": 2.7370030581039755,
      "grad_norm": 0.006160194519907236,
      "learning_rate": 1.7635066258919472e-06,
      "loss": 0.0002,
      "step": 1790
    },
    {
      "epoch": 2.7522935779816513,
      "grad_norm": 0.005880624055862427,
      "learning_rate": 1.6615698267074414e-06,
      "loss": 0.0002,
      "step": 1800
    },
    {
      "epoch": 2.767584097859327,
      "grad_norm": 0.005168571136891842,
      "learning_rate": 1.559633027522936e-06,
      "loss": 0.0002,
      "step": 1810
    },
    {
      "epoch": 2.782874617737003,
      "grad_norm": 0.00865477230399847,
      "learning_rate": 1.4576962283384302e-06,
      "loss": 0.0002,
      "step": 1820
    },
    {
      "epoch": 2.7981651376146788,
      "grad_norm": 0.005911597982048988,
      "learning_rate": 1.3557594291539246e-06,
      "loss": 0.0002,
      "step": 1830
    },
    {
      "epoch": 2.8134556574923546,
      "grad_norm": 0.005930634681135416,
      "learning_rate": 1.253822629969419e-06,
      "loss": 0.0002,
      "step": 1840
    },
    {
      "epoch": 2.8287461773700304,
      "grad_norm": 0.0067670466378331184,
      "learning_rate": 1.1518858307849134e-06,
      "loss": 0.0002,
      "step": 1850
    },
    {
      "epoch": 2.8440366972477067,
      "grad_norm": 0.008476627990603447,
      "learning_rate": 1.0499490316004079e-06,
      "loss": 0.0002,
      "step": 1860
    },
    {
      "epoch": 2.859327217125382,
      "grad_norm": 0.005380348768085241,
      "learning_rate": 9.480122324159022e-07,
      "loss": 0.0002,
      "step": 1870
    },
    {
      "epoch": 2.8746177370030583,
      "grad_norm": 0.005403880495578051,
      "learning_rate": 8.460754332313966e-07,
      "loss": 0.0002,
      "step": 1880
    },
    {
      "epoch": 2.8899082568807337,
      "grad_norm": 0.006239491980522871,
      "learning_rate": 7.441386340468911e-07,
      "loss": 0.0002,
      "step": 1890
    },
    {
      "epoch": 2.90519877675841,
      "grad_norm": 0.005619784817099571,
      "learning_rate": 6.422018348623854e-07,
      "loss": 0.0002,
      "step": 1900
    },
    {
      "epoch": 2.9204892966360854,
      "grad_norm": 0.005361687391996384,
      "learning_rate": 5.402650356778798e-07,
      "loss": 0.0002,
      "step": 1910
    },
    {
      "epoch": 2.9357798165137616,
      "grad_norm": 0.0071502747014164925,
      "learning_rate": 4.3832823649337413e-07,
      "loss": 0.0002,
      "step": 1920
    },
    {
      "epoch": 2.9510703363914375,
      "grad_norm": 0.006295382045209408,
      "learning_rate": 3.363914373088685e-07,
      "loss": 0.0002,
      "step": 1930
    },
    {
      "epoch": 2.9663608562691133,
      "grad_norm": 0.005341924726963043,
      "learning_rate": 2.344546381243629e-07,
      "loss": 0.0002,
      "step": 1940
    },
    {
      "epoch": 2.981651376146789,
      "grad_norm": 0.005739648826420307,
      "learning_rate": 1.325178389398573e-07,
      "loss": 0.0002,
      "step": 1950
    },
    {
      "epoch": 2.996941896024465,
      "grad_norm": 0.007607620675116777,
      "learning_rate": 3.0581039755351686e-08,
      "loss": 0.0002,
      "step": 1960
    }
  ],
  "logging_steps": 10,
  "max_steps": 1962,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3121034179903488.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
