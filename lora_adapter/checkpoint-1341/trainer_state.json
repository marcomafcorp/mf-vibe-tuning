{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1341,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02237136465324385,
      "grad_norm": 31.281105041503906,
      "learning_rate": 1.986577181208054e-05,
      "loss": 13.2136,
      "step": 10
    },
    {
      "epoch": 0.0447427293064877,
      "grad_norm": 35.45231246948242,
      "learning_rate": 1.9716629381058913e-05,
      "loss": 13.6743,
      "step": 20
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 10.280590057373047,
      "learning_rate": 1.9567486950037287e-05,
      "loss": 13.2062,
      "step": 30
    },
    {
      "epoch": 0.0894854586129754,
      "grad_norm": 45.1512565612793,
      "learning_rate": 1.941834451901566e-05,
      "loss": 13.3691,
      "step": 40
    },
    {
      "epoch": 0.11185682326621924,
      "grad_norm": 49.030757904052734,
      "learning_rate": 1.9269202087994035e-05,
      "loss": 12.9099,
      "step": 50
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 16.060882568359375,
      "learning_rate": 1.912005965697241e-05,
      "loss": 12.0304,
      "step": 60
    },
    {
      "epoch": 0.15659955257270694,
      "grad_norm": 43.14300537109375,
      "learning_rate": 1.8970917225950784e-05,
      "loss": 11.7342,
      "step": 70
    },
    {
      "epoch": 0.1789709172259508,
      "grad_norm": 19.418170928955078,
      "learning_rate": 1.882177479492916e-05,
      "loss": 11.6713,
      "step": 80
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 21.88505744934082,
      "learning_rate": 1.8672632363907532e-05,
      "loss": 11.4209,
      "step": 90
    },
    {
      "epoch": 0.22371364653243847,
      "grad_norm": 19.699514389038086,
      "learning_rate": 1.8523489932885906e-05,
      "loss": 9.2751,
      "step": 100
    },
    {
      "epoch": 0.24608501118568232,
      "grad_norm": 93.45620727539062,
      "learning_rate": 1.8374347501864283e-05,
      "loss": 8.7418,
      "step": 110
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 91.3456802368164,
      "learning_rate": 1.8225205070842654e-05,
      "loss": 9.302,
      "step": 120
    },
    {
      "epoch": 0.29082774049217003,
      "grad_norm": 76.97457122802734,
      "learning_rate": 1.8076062639821032e-05,
      "loss": 9.1313,
      "step": 130
    },
    {
      "epoch": 0.3131991051454139,
      "grad_norm": 30.169212341308594,
      "learning_rate": 1.7926920208799406e-05,
      "loss": 7.7942,
      "step": 140
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 25.07337188720703,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 7.1293,
      "step": 150
    },
    {
      "epoch": 0.3579418344519016,
      "grad_norm": 35.44023132324219,
      "learning_rate": 1.7628635346756154e-05,
      "loss": 6.0671,
      "step": 160
    },
    {
      "epoch": 0.38031319910514544,
      "grad_norm": 22.028303146362305,
      "learning_rate": 1.7479492915734528e-05,
      "loss": 5.4093,
      "step": 170
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 88.66267395019531,
      "learning_rate": 1.7330350484712902e-05,
      "loss": 5.3176,
      "step": 180
    },
    {
      "epoch": 0.4250559284116331,
      "grad_norm": 33.76445770263672,
      "learning_rate": 1.7181208053691277e-05,
      "loss": 4.2052,
      "step": 190
    },
    {
      "epoch": 0.44742729306487694,
      "grad_norm": 18.80291748046875,
      "learning_rate": 1.703206562266965e-05,
      "loss": 4.4184,
      "step": 200
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 47.8183479309082,
      "learning_rate": 1.6882923191648025e-05,
      "loss": 3.4824,
      "step": 210
    },
    {
      "epoch": 0.49217002237136465,
      "grad_norm": 26.811655044555664,
      "learning_rate": 1.67337807606264e-05,
      "loss": 2.9083,
      "step": 220
    },
    {
      "epoch": 0.5145413870246085,
      "grad_norm": 13.605729103088379,
      "learning_rate": 1.6584638329604773e-05,
      "loss": 2.9821,
      "step": 230
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 13.307345390319824,
      "learning_rate": 1.643549589858315e-05,
      "loss": 2.252,
      "step": 240
    },
    {
      "epoch": 0.5592841163310962,
      "grad_norm": 16.59859848022461,
      "learning_rate": 1.628635346756152e-05,
      "loss": 2.2426,
      "step": 250
    },
    {
      "epoch": 0.5816554809843401,
      "grad_norm": 20.050073623657227,
      "learning_rate": 1.6137211036539895e-05,
      "loss": 1.2353,
      "step": 260
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 7.895800590515137,
      "learning_rate": 1.5988068605518273e-05,
      "loss": 0.8229,
      "step": 270
    },
    {
      "epoch": 0.6263982102908278,
      "grad_norm": 2.1985414028167725,
      "learning_rate": 1.5838926174496644e-05,
      "loss": 0.6942,
      "step": 280
    },
    {
      "epoch": 0.6487695749440716,
      "grad_norm": 0.9334547519683838,
      "learning_rate": 1.568978374347502e-05,
      "loss": 0.5792,
      "step": 290
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 3.683175802230835,
      "learning_rate": 1.5540641312453395e-05,
      "loss": 0.4871,
      "step": 300
    },
    {
      "epoch": 0.6935123042505593,
      "grad_norm": 0.6505107283592224,
      "learning_rate": 1.5391498881431766e-05,
      "loss": 0.4324,
      "step": 310
    },
    {
      "epoch": 0.7158836689038032,
      "grad_norm": 0.6518688201904297,
      "learning_rate": 1.5242356450410144e-05,
      "loss": 0.4242,
      "step": 320
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 0.6676502227783203,
      "learning_rate": 1.5093214019388518e-05,
      "loss": 0.3769,
      "step": 330
    },
    {
      "epoch": 0.7606263982102909,
      "grad_norm": 0.6011627912521362,
      "learning_rate": 1.494407158836689e-05,
      "loss": 0.3502,
      "step": 340
    },
    {
      "epoch": 0.7829977628635347,
      "grad_norm": 1.8195338249206543,
      "learning_rate": 1.4794929157345266e-05,
      "loss": 0.3248,
      "step": 350
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 0.6623006463050842,
      "learning_rate": 1.464578672632364e-05,
      "loss": 0.2982,
      "step": 360
    },
    {
      "epoch": 0.8277404921700223,
      "grad_norm": 0.6880428791046143,
      "learning_rate": 1.4496644295302014e-05,
      "loss": 0.2964,
      "step": 370
    },
    {
      "epoch": 0.8501118568232662,
      "grad_norm": 0.706935703754425,
      "learning_rate": 1.4347501864280388e-05,
      "loss": 0.243,
      "step": 380
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 0.737067699432373,
      "learning_rate": 1.4198359433258764e-05,
      "loss": 0.2122,
      "step": 390
    },
    {
      "epoch": 0.8948545861297539,
      "grad_norm": 0.687058687210083,
      "learning_rate": 1.4049217002237137e-05,
      "loss": 0.1816,
      "step": 400
    },
    {
      "epoch": 0.9172259507829977,
      "grad_norm": 1.4970262050628662,
      "learning_rate": 1.3900074571215511e-05,
      "loss": 0.153,
      "step": 410
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 0.7119380831718445,
      "learning_rate": 1.3750932140193887e-05,
      "loss": 0.1261,
      "step": 420
    },
    {
      "epoch": 0.9619686800894854,
      "grad_norm": 0.5787431597709656,
      "learning_rate": 1.3601789709172261e-05,
      "loss": 0.1019,
      "step": 430
    },
    {
      "epoch": 0.9843400447427293,
      "grad_norm": 0.6131352782249451,
      "learning_rate": 1.3452647278150635e-05,
      "loss": 0.0807,
      "step": 440
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 0.3828469514846802,
      "learning_rate": 1.3303504847129009e-05,
      "loss": 0.064,
      "step": 450
    },
    {
      "epoch": 1.029082774049217,
      "grad_norm": 0.5022309422492981,
      "learning_rate": 1.3154362416107385e-05,
      "loss": 0.0532,
      "step": 460
    },
    {
      "epoch": 1.0514541387024607,
      "grad_norm": 0.248710498213768,
      "learning_rate": 1.3005219985085757e-05,
      "loss": 0.0465,
      "step": 470
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 0.560916543006897,
      "learning_rate": 1.2856077554064133e-05,
      "loss": 0.0431,
      "step": 480
    },
    {
      "epoch": 1.0961968680089484,
      "grad_norm": 0.13987745344638824,
      "learning_rate": 1.2706935123042507e-05,
      "loss": 0.0407,
      "step": 490
    },
    {
      "epoch": 1.1185682326621924,
      "grad_norm": 0.12151173502206802,
      "learning_rate": 1.255779269202088e-05,
      "loss": 0.0393,
      "step": 500
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 0.1040361076593399,
      "learning_rate": 1.2408650260999256e-05,
      "loss": 0.0384,
      "step": 510
    },
    {
      "epoch": 1.1633109619686801,
      "grad_norm": 0.09230474382638931,
      "learning_rate": 1.225950782997763e-05,
      "loss": 0.0378,
      "step": 520
    },
    {
      "epoch": 1.1856823266219239,
      "grad_norm": 0.090634286403656,
      "learning_rate": 1.2110365398956004e-05,
      "loss": 0.0377,
      "step": 530
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 0.10553685575723648,
      "learning_rate": 1.1961222967934378e-05,
      "loss": 0.037,
      "step": 540
    },
    {
      "epoch": 1.2304250559284116,
      "grad_norm": 0.08591688424348831,
      "learning_rate": 1.1812080536912754e-05,
      "loss": 0.0366,
      "step": 550
    },
    {
      "epoch": 1.2527964205816555,
      "grad_norm": 0.08187433332204819,
      "learning_rate": 1.1662938105891126e-05,
      "loss": 0.0362,
      "step": 560
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 0.07994237542152405,
      "learning_rate": 1.15137956748695e-05,
      "loss": 0.036,
      "step": 570
    },
    {
      "epoch": 1.2975391498881432,
      "grad_norm": 0.07842416316270828,
      "learning_rate": 1.1364653243847876e-05,
      "loss": 0.0357,
      "step": 580
    },
    {
      "epoch": 1.319910514541387,
      "grad_norm": 0.07893893867731094,
      "learning_rate": 1.1215510812826249e-05,
      "loss": 0.0354,
      "step": 590
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 0.07850722223520279,
      "learning_rate": 1.1066368381804625e-05,
      "loss": 0.0352,
      "step": 600
    },
    {
      "epoch": 1.3646532438478747,
      "grad_norm": 0.17565560340881348,
      "learning_rate": 1.0917225950782999e-05,
      "loss": 0.04,
      "step": 610
    },
    {
      "epoch": 1.3870246085011186,
      "grad_norm": 0.07941675931215286,
      "learning_rate": 1.0768083519761373e-05,
      "loss": 0.0347,
      "step": 620
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 0.08563246577978134,
      "learning_rate": 1.0618941088739747e-05,
      "loss": 0.0344,
      "step": 630
    },
    {
      "epoch": 1.4317673378076063,
      "grad_norm": 0.07904031872749329,
      "learning_rate": 1.0469798657718123e-05,
      "loss": 0.0342,
      "step": 640
    },
    {
      "epoch": 1.45413870246085,
      "grad_norm": 0.08073259145021439,
      "learning_rate": 1.0320656226696495e-05,
      "loss": 0.034,
      "step": 650
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 0.07504749298095703,
      "learning_rate": 1.017151379567487e-05,
      "loss": 0.0338,
      "step": 660
    },
    {
      "epoch": 1.4988814317673378,
      "grad_norm": 0.08446155488491058,
      "learning_rate": 1.0022371364653245e-05,
      "loss": 0.0335,
      "step": 670
    },
    {
      "epoch": 1.5212527964205815,
      "grad_norm": 0.08250387012958527,
      "learning_rate": 9.87322893363162e-06,
      "loss": 0.0332,
      "step": 680
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 0.0792108029127121,
      "learning_rate": 9.724086502609993e-06,
      "loss": 0.033,
      "step": 690
    },
    {
      "epoch": 1.5659955257270695,
      "grad_norm": 0.0786147341132164,
      "learning_rate": 9.574944071588368e-06,
      "loss": 0.0327,
      "step": 700
    },
    {
      "epoch": 1.5883668903803132,
      "grad_norm": 0.07787210494279861,
      "learning_rate": 9.425801640566742e-06,
      "loss": 0.0325,
      "step": 710
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 0.08066698908805847,
      "learning_rate": 9.276659209545118e-06,
      "loss": 0.0322,
      "step": 720
    },
    {
      "epoch": 1.633109619686801,
      "grad_norm": 0.07859714329242706,
      "learning_rate": 9.12751677852349e-06,
      "loss": 0.0321,
      "step": 730
    },
    {
      "epoch": 1.6554809843400449,
      "grad_norm": 0.07767807692289352,
      "learning_rate": 8.978374347501864e-06,
      "loss": 0.0318,
      "step": 740
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 0.08098865300416946,
      "learning_rate": 8.82923191648024e-06,
      "loss": 0.0316,
      "step": 750
    },
    {
      "epoch": 1.7002237136465324,
      "grad_norm": 0.0790858194231987,
      "learning_rate": 8.680089485458614e-06,
      "loss": 0.0314,
      "step": 760
    },
    {
      "epoch": 1.7225950782997763,
      "grad_norm": 0.08423792570829391,
      "learning_rate": 8.530947054436988e-06,
      "loss": 0.0312,
      "step": 770
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 0.0914614275097847,
      "learning_rate": 8.381804623415362e-06,
      "loss": 0.031,
      "step": 780
    },
    {
      "epoch": 1.767337807606264,
      "grad_norm": 0.09699403494596481,
      "learning_rate": 8.232662192393737e-06,
      "loss": 0.0307,
      "step": 790
    },
    {
      "epoch": 1.7897091722595078,
      "grad_norm": 0.13321015238761902,
      "learning_rate": 8.08351976137211e-06,
      "loss": 0.0304,
      "step": 800
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 0.2764340341091156,
      "learning_rate": 7.934377330350485e-06,
      "loss": 0.03,
      "step": 810
    },
    {
      "epoch": 1.8344519015659957,
      "grad_norm": 0.5948523879051208,
      "learning_rate": 7.785234899328859e-06,
      "loss": 0.029,
      "step": 820
    },
    {
      "epoch": 1.8568232662192394,
      "grad_norm": 0.49439772963523865,
      "learning_rate": 7.636092468307233e-06,
      "loss": 0.0232,
      "step": 830
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 0.5705463886260986,
      "learning_rate": 7.486950037285609e-06,
      "loss": 0.0187,
      "step": 840
    },
    {
      "epoch": 1.901565995525727,
      "grad_norm": 0.2004740983247757,
      "learning_rate": 7.337807606263982e-06,
      "loss": 0.0183,
      "step": 850
    },
    {
      "epoch": 1.9239373601789709,
      "grad_norm": 0.24669422209262848,
      "learning_rate": 7.188665175242357e-06,
      "loss": 0.0173,
      "step": 860
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 0.22087319195270538,
      "learning_rate": 7.039522744220731e-06,
      "loss": 0.0189,
      "step": 870
    },
    {
      "epoch": 1.9686800894854586,
      "grad_norm": 0.555247962474823,
      "learning_rate": 6.8903803131991054e-06,
      "loss": 0.0162,
      "step": 880
    },
    {
      "epoch": 1.9910514541387023,
      "grad_norm": 0.3394467830657959,
      "learning_rate": 6.7412378821774804e-06,
      "loss": 0.0157,
      "step": 890
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 0.17440958321094513,
      "learning_rate": 6.5920954511558546e-06,
      "loss": 0.015,
      "step": 900
    },
    {
      "epoch": 2.0357941834451903,
      "grad_norm": 0.2447061985731125,
      "learning_rate": 6.442953020134229e-06,
      "loss": 0.0145,
      "step": 910
    },
    {
      "epoch": 2.058165548098434,
      "grad_norm": 0.15056827664375305,
      "learning_rate": 6.293810589112604e-06,
      "loss": 0.0143,
      "step": 920
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 0.15474887192249298,
      "learning_rate": 6.144668158090977e-06,
      "loss": 0.0137,
      "step": 930
    },
    {
      "epoch": 2.1029082774049215,
      "grad_norm": 0.1848393827676773,
      "learning_rate": 5.995525727069351e-06,
      "loss": 0.0134,
      "step": 940
    },
    {
      "epoch": 2.1252796420581657,
      "grad_norm": 0.24631699919700623,
      "learning_rate": 5.846383296047726e-06,
      "loss": 0.0129,
      "step": 950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 0.15086230635643005,
      "learning_rate": 5.6972408650261e-06,
      "loss": 0.0125,
      "step": 960
    },
    {
      "epoch": 2.170022371364653,
      "grad_norm": 0.1385279893875122,
      "learning_rate": 5.548098434004475e-06,
      "loss": 0.0119,
      "step": 970
    },
    {
      "epoch": 2.192393736017897,
      "grad_norm": 0.30312830209732056,
      "learning_rate": 5.398956002982849e-06,
      "loss": 0.0116,
      "step": 980
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 1.0035557746887207,
      "learning_rate": 5.2498135719612235e-06,
      "loss": 0.0113,
      "step": 990
    },
    {
      "epoch": 2.237136465324385,
      "grad_norm": 0.3182443380355835,
      "learning_rate": 5.1006711409395985e-06,
      "loss": 0.011,
      "step": 1000
    },
    {
      "epoch": 2.2595078299776286,
      "grad_norm": 0.36351341009140015,
      "learning_rate": 4.951528709917972e-06,
      "loss": 0.0106,
      "step": 1010
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 0.2269618958234787,
      "learning_rate": 4.802386278896347e-06,
      "loss": 0.0102,
      "step": 1020
    },
    {
      "epoch": 2.3042505592841165,
      "grad_norm": 0.4835628569126129,
      "learning_rate": 4.653243847874721e-06,
      "loss": 0.01,
      "step": 1030
    },
    {
      "epoch": 2.3266219239373602,
      "grad_norm": 0.4195263683795929,
      "learning_rate": 4.504101416853095e-06,
      "loss": 0.0098,
      "step": 1040
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 0.5650768876075745,
      "learning_rate": 4.354958985831469e-06,
      "loss": 0.01,
      "step": 1050
    },
    {
      "epoch": 2.3713646532438477,
      "grad_norm": 0.423511266708374,
      "learning_rate": 4.205816554809844e-06,
      "loss": 0.0096,
      "step": 1060
    },
    {
      "epoch": 2.393736017897092,
      "grad_norm": 0.24099954962730408,
      "learning_rate": 4.056674123788218e-06,
      "loss": 0.0092,
      "step": 1070
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 0.1704355627298355,
      "learning_rate": 3.907531692766592e-06,
      "loss": 0.0088,
      "step": 1080
    },
    {
      "epoch": 2.4384787472035794,
      "grad_norm": 0.12237067520618439,
      "learning_rate": 3.758389261744967e-06,
      "loss": 0.0087,
      "step": 1090
    },
    {
      "epoch": 2.460850111856823,
      "grad_norm": 0.1331782341003418,
      "learning_rate": 3.6092468307233407e-06,
      "loss": 0.0088,
      "step": 1100
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 0.5406557321548462,
      "learning_rate": 3.4601043997017152e-06,
      "loss": 0.0084,
      "step": 1110
    },
    {
      "epoch": 2.505592841163311,
      "grad_norm": 0.6746283769607544,
      "learning_rate": 3.3109619686800898e-06,
      "loss": 0.0084,
      "step": 1120
    },
    {
      "epoch": 2.527964205816555,
      "grad_norm": 0.28519076108932495,
      "learning_rate": 3.1618195376584643e-06,
      "loss": 0.0082,
      "step": 1130
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 0.4143466055393219,
      "learning_rate": 3.012677106636838e-06,
      "loss": 0.0079,
      "step": 1140
    },
    {
      "epoch": 2.5727069351230423,
      "grad_norm": 0.3435768783092499,
      "learning_rate": 2.8635346756152126e-06,
      "loss": 0.0077,
      "step": 1150
    },
    {
      "epoch": 2.5950782997762865,
      "grad_norm": 0.12393946200609207,
      "learning_rate": 2.714392244593587e-06,
      "loss": 0.0076,
      "step": 1160
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 0.1339818239212036,
      "learning_rate": 2.5652498135719617e-06,
      "loss": 0.0075,
      "step": 1170
    },
    {
      "epoch": 2.639821029082774,
      "grad_norm": 0.6729848980903625,
      "learning_rate": 2.416107382550336e-06,
      "loss": 0.0074,
      "step": 1180
    },
    {
      "epoch": 2.662192393736018,
      "grad_norm": 0.1973750740289688,
      "learning_rate": 2.26696495152871e-06,
      "loss": 0.0072,
      "step": 1190
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 0.22273831069469452,
      "learning_rate": 2.117822520507084e-06,
      "loss": 0.0073,
      "step": 1200
    },
    {
      "epoch": 2.7069351230425056,
      "grad_norm": 0.2876267433166504,
      "learning_rate": 1.9686800894854587e-06,
      "loss": 0.0071,
      "step": 1210
    },
    {
      "epoch": 2.7293064876957494,
      "grad_norm": 0.12838168442249298,
      "learning_rate": 1.819537658463833e-06,
      "loss": 0.007,
      "step": 1220
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 0.39023852348327637,
      "learning_rate": 1.6703952274422076e-06,
      "loss": 0.0073,
      "step": 1230
    },
    {
      "epoch": 2.7740492170022373,
      "grad_norm": 0.3526950478553772,
      "learning_rate": 1.5212527964205817e-06,
      "loss": 0.0069,
      "step": 1240
    },
    {
      "epoch": 2.796420581655481,
      "grad_norm": 1.0207279920578003,
      "learning_rate": 1.3721103653989563e-06,
      "loss": 0.0069,
      "step": 1250
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 0.8807070255279541,
      "learning_rate": 1.2229679343773304e-06,
      "loss": 0.0067,
      "step": 1260
    },
    {
      "epoch": 2.841163310961969,
      "grad_norm": 0.7935832142829895,
      "learning_rate": 1.0738255033557048e-06,
      "loss": 0.0072,
      "step": 1270
    },
    {
      "epoch": 2.8635346756152127,
      "grad_norm": 0.36918848752975464,
      "learning_rate": 9.246830723340791e-07,
      "loss": 0.0067,
      "step": 1280
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 1.1837433576583862,
      "learning_rate": 7.755406413124535e-07,
      "loss": 0.0068,
      "step": 1290
    },
    {
      "epoch": 2.9082774049217,
      "grad_norm": 0.5912693738937378,
      "learning_rate": 6.263982102908278e-07,
      "loss": 0.0068,
      "step": 1300
    },
    {
      "epoch": 2.930648769574944,
      "grad_norm": 0.15438687801361084,
      "learning_rate": 4.772557792692021e-07,
      "loss": 0.0065,
      "step": 1310
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 0.5959910154342651,
      "learning_rate": 3.2811334824757647e-07,
      "loss": 0.0066,
      "step": 1320
    },
    {
      "epoch": 2.975391498881432,
      "grad_norm": 0.20715142786502838,
      "learning_rate": 1.789709172259508e-07,
      "loss": 0.0067,
      "step": 1330
    },
    {
      "epoch": 2.9977628635346756,
      "grad_norm": 0.43363267183303833,
      "learning_rate": 2.982848620432513e-08,
      "loss": 0.0067,
      "step": 1340
    }
  ],
  "logging_steps": 10,
  "max_steps": 1341,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2133183911952384.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
